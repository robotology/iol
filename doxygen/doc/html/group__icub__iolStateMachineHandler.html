<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>iol: State Machine Handler</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">iol
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">State Machine Handler</div>  </div>
</div><!--header-->
<div class="contents">

<p>The module managing all the components of the IOL application.  
</p>
<p>The module managing all the components of the IOL application. </p>
<h1><a class="anchor" id="intro_sec"></a>
Description</h1>
<p>This module is responsible for coordinating all the components that form the overall <a class="el" href="group__icub__iolStateMachineHandler.html">State Machine Handler</a> application. To this end, it receives input from the human operator and then forwards proper requests to the classifier, the blob detector, the motion caputer, the motor layer to let the robot achieve the goal.</p>
<p>The commands sent as bottles to the module port /&lt;modName&gt;/human:rpc are the following:</p>
<p>(notation: [.] identifies a vocab, &lt;.&gt; specifies a number, "." specifies a string)</p>
<p><b>HOME</b> <br  />
format: [home] <br  />
action: brings the robot back to its resting state.</p>
<p><b>CALIB_TABLE</b> <br  />
format: [cata] <br  />
action: let the robot discover the table height.</p>
<p><b>CALIB_KINEMATICS</b> <br  />
This command is splitted in two consecutive sub-commands:</p>
<p>format subcmd1: [caki] [start] [left]/[right] "object" <br  />
action: the robot reaches the object "object" with the specified hand and waits for the interaction with human based on force control in order to store the kinematic offsets corresponding to the given object.</p>
<p>format subcmd2: [caki] [stop] <br  />
action: terminate the calibration phase.</p>
<p><b>TRACK</b> <br  />
format: [track] [start]/[stop] <br  />
action: let the robot track any moving object.</p>
<p><b>NAME</b> <br  />
format: [name] "object" <br  />
action: let the robot learn a name to be associated with the object that was tracked right before.</p>
<p><b>FORGET</b> <br  />
format: [forget] "object" <br  />
action: remove the object from the internal memory. <br  />
The special key "all" is used to purge the whole memory.</p>
<p><b>WHERE</b> <br  />
format: [where] "object" <br  />
action: ask the robot to point at the given object. <br  />
If no_object/wrong is recognized then the robot enters the learning phase, where further commands are envisaged: i.e. [ack], [nack], [skip], ...</p>
<p><b>WHAT</b> <br  />
format: [what] <br  />
action: ask the robot to say the name of the pointed object. <br  />
In case of a mistake the robot enters the learning phase where further commands are envisaged: i.e. [ack], [nack], [skip], [name], ...</p>
<p><b>THIS</b> <br  />
format: [this] "object" ["click"] <br  />
action: tell the robot the name of the pointed object. <br  />
In case the option "click" is specified, then the location clicked in the clickable viewer will be used rather than the one resulting from the pointing action.</p>
<p><b>EXPLORE</b> <br  />
format: [explore] "object" <br  />
action: let the robot explore the object from many different view points in order to improve its knowledge.</p>
<p><b>REINFORCE</b> <br  />
format: [reinforce] "object" (&lt;x&gt; &lt;y&gt; &lt;z&gt;) <br  />
action: let the robot improve the recognition rate of the specified object whose 3d coordinates are provided.</p>
<p><b>MOTOR_COMMANDS</b> <br  />
format: [take]/[grasp]/[push]/[touch]/[hold]/[drop] "object" <br  />
action: ask the robot to perform some motor commands on the given object.</p>
<p><b>ATTENTION</b> <br  />
format: [attention] [start]/[stop] <br  />
action: switch on/off the attention system.</p>
<p><b>SAY</b> <br  />
format: [say] "sentence" <br  />
action: let the robot utter the specified sentence.</p>
<p>The commands sent as bottles to the module port /&lt;modName&gt;/rpc are the following:</p>
<p>(notation: [.] identifies a vocab, &lt;.&gt; specifies a double, "." specifies a string)</p>
<p><b>STATUS</b> <br  />
format: [status] <br  />
reply: "ack" "idle" | "busy" whether an action is currently being processed or not.</p>
<h1><a class="anchor" id="lib_sec"></a>
Libraries</h1>
<ul>
<li>YARP libraries.</li>
</ul>
<h1><a class="anchor" id="portsc_sec"></a>
Ports Created</h1>
<ul>
<li><em>/</em> &lt;modName&gt;/img:i receives the image acquired from the camera previously specified through the command-line parameters.</li>
<li><em>/</em> &lt;modName&gt;/img:o streams out the image containing recognized object. The image is updated whenever an action is required to be executed.</li>
<li><em>/</em> &lt;modName&gt;/imgLoc:o streams out the images for real-time objects localization.</li>
<li><em>/</em> &lt;modName&gt;/imgHistogram:o streams out the histogram of classification scores.</li>
<li><em>/</em> &lt;modName&gt;/histObjLocation:i receives the pixel in the image from which retrieve a cartesian position for displaying the classification histograms of the closest blob.</li>
<li><em>/</em> &lt;modName&gt;/recog:o streams out information about the object just recognized; the format is: ("label" "object-name") ("position_3d" (&lt;x&gt; &lt;y&gt; &lt;z&gt;)) ("type" "recognition"|"creation").</li>
<li><em>/</em> &lt;modName&gt;/imgClassifier:o used to pass images to the classifier.</li>
<li><em>/</em> &lt;modName&gt;/human:rpc receives requests for actions execution.</li>
<li><em>/</em> &lt;modName&gt;/rpc receives check requests.</li>
<li><em>/</em> &lt;modName&gt;/blobs:rpc used to forward requests to the blob detector for image segmentation.</li>
<li><em>/</em> &lt;modName&gt;/classify:rpc sends out requests for object classification.</li>
<li><em>/</em> &lt;modName&gt;/motor:rpc sends out motor commands.</li>
<li><em>/</em> &lt;modName&gt;/motor_stop:rpc used to interrupt/restore motor capabilities.</li>
<li><em>/</em> &lt;modName&gt;/motor_stop:i receives the [icub-stop] trigger from the verbal commands interpreter.</li>
<li><em>/</em> &lt;modName&gt;/point:i receives the latest pointed location within the image from the motion capture module.</li>
<li><em>/</em> &lt;modName&gt;/speak:o streams out the robot's sentences that need to be spoken.</li>
<li><em>/</em> &lt;modName&gt;/memory:rpc used to communicate with the objects properties collector.</li>
</ul>
<h1><a class="anchor" id="parameters_sec"></a>
Parameters</h1>
<p>&ndash;name <em>name</em> </p><ul>
<li>specify the module stem-name, which is <em>iolStateMachineHandler</em> by default. The stem-name is used as prefix for all open ports.</li>
</ul>
<p>&ndash;rt_localization_period <em>period</em> </p><ul>
<li>specify the period (given in [ms]) of the thread devoted to real-time objects localization. The default value is 30 ms.</li>
</ul>
<p>&ndash;exploration_period <em>period</em> </p><ul>
<li>specify the period (given in [ms]) of the thread devoted to images acquisition during objects exploration phase. The default value is 30 ms.</li>
</ul>
<p>&ndash;memory_update_period <em>period</em> </p><ul>
<li>specify the period (given in [ms]) of the thread devoted to updating the objects properties database. The default value is 60 ms.</li>
</ul>
<p>&ndash;blobs_detection_timeout <em>tmo</em> </p><ul>
<li>specify the timeout (given in [s]) for disabling blob detection. The default valus is 0.2 s.</li>
</ul>
<p>&ndash;camera <em></em>[left|right]</p><ul>
<li>specify the camera used to localized object. The default camera is "left".</li>
</ul>
<p>&ndash;skim_blobs_x_bounds <em>(min max)</em></p><ul>
<li>to reduce blob detection within the given x bounds on the table.</li>
</ul>
<p>&ndash;skim_blobs_y_bounds <em>(min max)</em></p><ul>
<li>to reduce blob detection within the given y bounds on the table.</li>
</ul>
<p>&ndash;classification_threshold <em>t</em> </p><ul>
<li>to establish the default threshold <em>t</em> used by the classification algorithm.</li>
</ul>
<p>&ndash;improve_train_period <em>T</em> </p><ul>
<li>to extend the training instance of <em>T</em> seconds within which collect further relevant images.</li>
</ul>
<p>&ndash;train_flipped_images <em></em>[on|off]</p><ul>
<li>allow training over flipped images to improve accuracy; default is "off".</li>
</ul>
<p>&ndash;train_burst_images <em></em>[on|off]</p><ul>
<li>allow acquiring a burst of images over which carry out training later on; default is "off".</li>
</ul>
<p>&ndash;skip_learning_upon_success <em></em>[on|off]</p><ul>
<li>avoid refining the objects knowledge when the recognition is successful; default is "off".</li>
</ul>
<p>&ndash;hist_filter_length <em>len</em> </p><ul>
<li>allow selecting the length of the moving average filter used to smooth out the quickly varying scores.</li>
</ul>
<p>&ndash;block_eyes <em>ver</em> </p><ul>
<li>for <em>ver&gt;0</em> specify to block eyes vergence at <em>ver</em> angle before gazing at the object to power-grasp.</li>
</ul>
<p>&ndash;drop_position <em> (x y z) </em></p><ul>
<li>if specified, force dropping the object at a given location.</li>
</ul>
<p>&ndash;tracker_type <em>type</em> </p><ul>
<li>specify the opencv tracker type. Default value is <em>BOOSTING</em>.</li>
</ul>
<p>&ndash;tracker_timeout <em>tmo</em> </p><ul>
<li>specify the timeout (given in [s]) for stopping tracking. Default value is 5.0 s.</li>
</ul>
<p>&ndash;tracker_min_blob_size <em>(min max)</em></p><ul>
<li>specify the minimum size of the blobs in terms of (width height) that can be tracked.</li>
</ul>
<p>&ndash;attention <em></em>[on|off]</p><ul>
<li>enable/disable attention system at startup (default is on).</li>
</ul>
<h1><a class="anchor" id="tested_os_sec"></a>
Tested OS</h1>
<p>Windows, Linux</p>
<dl class="section author"><dt>Author</dt><dd>Ugo Pattacini </dd></dl>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
